{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0ed29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743a9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6e4f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270000, 3124) (3124,)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "data_train = np.load('data_train.npy')\n",
    "labels_train = np.load('labels_train.npy')\n",
    "\n",
    "labels_names =['Stadium','Building','Traffic Sign','Forest','Flowers',\n",
    "              'Street','Classroom','Bridge','Statue','Lake']\n",
    "\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28fbcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2499, 270000)\n",
      "(625, 270000)\n",
      "(2499,)\n",
      "(625,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train.transpose(), labels_train, test_size=0.20)\n",
    "\n",
    "X_train, X_test, y_train, y_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "#X_train = np.resize(X_train,(2499,300,300,3))\n",
    "#X_test = np.resize(X_test,(625,300,300,3))\n",
    "#y_train_new = y_train - 1\n",
    "#y_test_new = y_test - 1\n",
    "#print(y_train_new)\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_new))\n",
    "#test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382f8c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (2499,)\n",
      "Shape after one-hot encoding:  (2499, 10)\n"
     ]
    }
   ],
   "source": [
    "# keras imports for the dataset and building our neural network\n",
    "#from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# loading the dataset\n",
    "#(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# # building the input vector from the 32x32 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 300, 300, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 300, 300, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train - 1\n",
    "y_test= y_test - 1\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "# # building a linear stack of layers with the sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # convolutional layer\n",
    "# model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(300, 300, 3)))\n",
    "\n",
    "# # convolutional layer\n",
    "# model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# # flatten output of conv\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # hidden layer\n",
    "# model.add(Dense(500, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(250, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# # output layer\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# # compiling the sequential model\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# # training the model for 10 epochs\n",
    "# model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054b592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.003921569\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb88788",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(300, 300, 3)),\n",
    "  layers.Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "  layers.MaxPool2D(pool_size=(2,2)),\n",
    "  layers.Dropout(0.25),\n",
    "  layers.Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "  layers.MaxPool2D(pool_size=(2,2)),\n",
    "  layers.Dropout(0.25),  \n",
    "  layers.Flatten(),\n",
    "  layers.Dense(500, activation='relu'),\n",
    "  layers.Dropout(0.4),\n",
    "  layers.Dense(250, activation='relu'),\n",
    "  layers.Dropout(0.3),\n",
    "  layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# model = Sequential([\n",
    "#   layers.Rescaling(1./255, input_shape=(300,300,3)),\n",
    "#   layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "#   layers.MaxPooling2D(),\n",
    "#   layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "#   layers.MaxPooling2D(),\n",
    "#   layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "#   layers.MaxPooling2D(),\n",
    "#   layers.Flatten(),\n",
    "#   layers.Dense(128, activation='relu'),\n",
    "#   layers.Dense(num_classes)\n",
    "# ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae8171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doron\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1965s 49s/step - loss: 5.5181 - accuracy: 0.1152 - val_loss: 2.3010 - val_accuracy: 0.1184\n",
      "Epoch 2/10\n",
      "34/40 [========================>.....] - ETA: 4:55 - loss: 2.2659 - accuracy: 0.1485"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data=test_dataset,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e43912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
